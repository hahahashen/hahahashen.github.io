---
layout: post
title: Raft论文阅读总结
category: learning
tags: raft
description: Raft论文阅读总结
---

### Figure2所描述的Raft算法

#### 状态集

作为所有server来说，**持久化状态**：

在应答RPC前，从非易失storage之中加载

**currentTerm：server遇见过的最新的term，一开始是0**

**votedFor**：在当前term中我投给的候选者id or null

**log[]**：log entrites；每个entry包含状态机命令以及term，一开始的索引是1

作为所有server来说，易失性状态

**commitIndex**：已提交的最高的log entry的index，一开始是0

lastApplied：已应用于状态机上的最高的log entry的索引。一开始是0

作为leader来说，易失性状态：

在选举后重新初始化

nextIndex[]：给每一个server记录的下一个将要发送给该server的entry index；一开始是leader last log index + 1

`nextIndex` is used for performance – you only need to send these things to this peer.

matchIndex[]：给每一个server记录的已知被**复制**了的最高的entry index 一开始是0

for each server, index of highest log entry known to be replicated on server (initialized to 0, increases monotonically)

使用nextIndex来控制entry发送；使用matchIndex来控制commitIndex的移动

#### 规则

**作为所有server来说**，

如果commitIndex>lastApplied，说明已经有新的提交了并且还没被应用，增加lastApplied，并将log[lastApplied]应用到状态机中

如果RPC请求或回复中包含的term T>currentTerm：set currentTerm = T, 转变为follower；因为我的term已经落后了，所以我也没有资格成为候选者，我只能成为follower等待着别人来更新我

作为Follower来说，

Respond to RPCs from candidates and leaders

如果**选举时钟超时**，在**本轮**选举时钟倒计时的过程中，我没有从当前leader接收AppendEntries RPC或给candidate投票：自己转换为候选人；我已经乖乖的成为follower了，但是没有leader或是候选人作为，那我自己来竞选

如果进入下一轮选举时钟，那么投票结果重置

![img](https://nankai.feishu.cn/space/api/box/stream/download/asynccode/?code=ZjI1Mjk4NWNkNmMyM2YwNjlmYTIyYTJlNGIwYmU2NDRfWjhIM3Nyc1hkZGlCQ092WnJoQ01uTHo5YVlUWjlEVllfVG9rZW46QVltRmJndnVOb0hDbjJ4d2g0MWNKWjgxblljXzE3MzE1ODU4NzQ6MTczMTU4OTQ3NF9WNA)

作为Candidates来说，

一旦转换为候选人，就开始选举：

- 增加currentTerm
- 为自己投票
- 重置选举计时器
- 发送RequestVote RPC到所有其他服务器

如果从大多数服务器收到投票：成为leader

如果从新的leader收到AppendEntries RPC，转换为follower

如果选举超时：开始新的选举

作为leader来说，

在赢得选举后，发送初始为空的AppendEntries RPCs也即心跳包给每一个server；即使无事可做也重复发送防止**选举时钟超时**

如果从客户端收到命令：将entry追加到本地log，在entry**应用**到状态机后响应客户端

If last log index **≥** nextIndex for a follower【leader会保存follower的nextIndex】: send

AppendEntries RPC with log entries starting at nextIndex【follower下一个需要entry[nextIndex],现在已经出现了，所以就着手发送；所以nextIndex一开始初始化为leader last log index + 1，相当于等待leader从客户端接收，然后发送】

• If successful: update nextIndex and matchIndex for follower

- matchIndex怎么更新，详见https://github.com/hahahashen/mit6824/commit/a79f98ceae46d5603f4faca6177f369d9
- 某个server的matchIndex更新规则为，如果AppendEntries包reply为true【见[Raft论文阅读](https://nankai.feishu.cn/wiki/NhUsw1BfIiV1RbkcTQ0cGM0unLe#share-TX73dyhuRoOaEuxec3AcEdPqnze)】，说明follower将RPC包中追加的entry加入到自己本地的log中了，于是更新matchIndex为prevLogIndex+len(Entries)

• If AppendEntries fails because of log inconsistency:decrement nextIndex and retry

• **If there exists an N such that N > commitIndex, a majority of** **matchIndex[i]** **≥ N【大多数都复制了】, and log[N].term** **==** **currentTerm:set commitIndex = N** 【leader不主动commit之前term的entry，而是在提交自己term的new entry后，顺带着连之前的大多数entry一起提交了】

#### RPC包

##### RequestVote RPC

这个包是候选者发出的，用来收集选票

term：候选者的term

candidateId： candidate requesting vote，候选者的serverID

lastLogIndex：index of candidate’s last log entry

lastLogTerm：term of candidate’s last log entry

返回结果：

term T：接收者的currentTerm, for candidate to update itself；【如果候选者的term低于接收者的currentTerm，那么set currentTerm = T, 转变为follower】

voteGranted true means candidate received vote【返回真意味着收到了选票】

接收到这个包的server的逻辑：

1、如果自己的currentTerm>包中的term，voteGranted为false

2、如果votedFor是null或者与包中的candidateId 相同【我没有投票给别人】，并且候选者的log至少跟接收者的log一样新，那么接收者就投票给该候选者，voteGranted 为真

怎么判断是不是一样新？：通过比较包中的lastLogTerm是否先于我，如果先于我，则新于我；

如果与我一样，则再比较lastLogIndex是否大于等于我，如果大于等于我，则跟我一样新或者比我新

##### AppendEntries RPC

当 leader 刚开始生效时，它将所有 nextIndex 初始化为它自己的最后一个 log entry 的下一个值

经过一次 rejection（请求拒绝），leader 会将该 follower 的 nextIndex 减少

这个包会由leader在分发log entry时发出。同时也被用作心跳包

包中携带的信息

**term** leader’s term

**leaderId** so follower can redirect clients，leader server Id

**prevLogIndex** index of log entry immediately **preceding** new ones 新的前一个

**prevLogTerm** term of prevLogIndex entry，**prevLogIndex** 对应的term值

**entries[]** log entries to store (empty for heartbeat;may send more than one for efficiency)

**leaderCommit** leader’s commitIndex

包的返回结果

**term** currentTerm, for leader to update itself，我自己的currentTerm

**success** true if follower contained entry matching prevLogIndex and prevLogTerm，如果follower包含了与prevLogIndex 、prevLogTerm 匹配的entry则回复true

接收者实现：

1. Reply false if term(leader) < currentTerm(自己的) 【不重置计时器】【但是当term(leader) >= currentTerm(自己的)，说明有没过期的leader出现了，**重置计时器** 】
2. Reply false if log doesn’t contain an entry at prevLogIndex whose term matches prevLogTerm 
3. If an existing entry conflicts with a new one (same index but different terms), delete the existing entry and all that follow it  删除所有矛盾的以及所有跟在他之后的
4. Append any new entries not already in the log 把不在log中的加进去
5. If leaderCommit > commitIndex, set commitIndex =**min**(leaderCommit, **index of last new entry**)

心跳包不更新commitIndex，因为没有new entry的复制过程，不确定follower在0-commitIndex之间的log entry与leader一致

心跳包可以更新commitIndex，即比较prevLogIndex是否一致，一致则取min(prevLogIndex,leaderCommit)进行更新

commitIndex更新详见[LAB BUG Case&Fix](https://nankai.feishu.cn/wiki/IhMDw5O56iUbeYkdFN5crMsdnwe)

commitIndex更新为min(leaderCommit,args.prevLogIndex+len(Entries))

**总结：commitIndex的更新**

1、心跳包是否要更新commitIndex

不更新，因为论文中写的 If leaderCommit > commitIndex, set commitIndex =

min(leaderCommit, index of **last new entry**)

如果都没有追加的new entry，那就没有需要更新的commitIndex

2、心跳包不更新commitIndex，那么caseA的没有entry驱动，部分实例的commitIndex没有得到及时更新的问题怎么办

使用特定Entry RPC包解决

在leader commitIndex更新后，给各实例发送一次带有最新commitIndex的Entry包

各实例在收到该包时，也要进行严格限制。只有在我的last log entry与leader在leaderCommit处的entry完全一致时，才更新我的commitIndex

3-1、各实例log长度落后leader的log，需要leader补全，有源源不断的Entry包发送过来更新commitIndex

3-2、各实例log长度领先leader的log，需要leader删除，有源源不断的Entry包发送过来更新commitIndex

3-3、各实例log长度等于leader的log：last Entry一致回到2情况，last Entry不一致回到3-2删除情况

4、各实例log长度等于leader的log：last Entry一致，commitIndex没得到更新会在什么时候出现（更新就不用管了）

一开始的实例接受了该last Entry，但是在leader看来还没有达到大多数更新条件；

当达到大多数更新条件；并且leader也更新了leaderCommitIndex却没有消息通知先前的实例时，先前的实例就得不到更新，于是有2情况以及补救策略

##### InstallSnapshot RPC

参数：

term：leader’s term

leaderId：so follower can redirect clients

lastIncludedIndex：the snapshot replaces all entries up through and including this index 快照生成到哪个index了，包括index本身

lastIncludedTerm：term of lastIncludedIndex

返回结果：

term currentTerm, for leader to update itself

1. Reply immediately if term < currentTerm
2. Create new snapshot file if first chunk (offset is 0)
3. Write data into snapshot file at given offset
4. Reply and wait for more data chunks if done is false
5. Save snapshot file, discard any existing or partial snapshot with a smaller index
6. If existing log entry has same index and term as snapshot’s last included entry, retain log entries following it and reply
7. Discard the entire log
8. Reset state machine using snapshot contents (and load snapshot’s cluster configuration)

上面是分块发送快照的RPC处理规则，mit6824中所要求的实现快照是不分块发送的，那么处理规则改成如下：

1. Reply immediately if term < currentTerm
2. 创建新的快照并保存，丢弃已存在的或者index更小的快照
3. 如果已经存在的log有和快照中相同index和term的entry，保留index之后的log entry并return
4. 如果上一步没有return，那么就丢弃整个log
5. 使用快照内容重置状态机(并加载快照的集群配置)

感觉还得加个比较 if rf.lastIncludedIndex<args.lastIncludedIndex,才保存leader发送来的快照，并进行后续操作

### Section5 要点总结

1、选举时钟的时间是随机的

这样在大多数case下，只有一个server超时发起选举，能够很快地就选出leader

如果每个server的选举时间的是一样的，那么他们会同时超时，同时发起选举，会导致选票分裂，你得2票，我得3票，他得3票，每个server获得的选票都不足以构成大多数

2、日志复制的一致性是可归纳递推的

如果两个server某个index的entry是一致的，那么说明他们在该index之前的所有entry都是一致的

因为RequestAppendEntries会携带new entry的前一个entry的信息，只有前一个一致了，才会追加后面，可以一步步地归纳递推到第一个entry

3、Raft的日志即使已经复制到大多数，也是有可能会被覆盖的，Figure8描述的问题

a、Term1：S1当选leader，接收一个new entry

b、Term2：S1还是当选一个leader，提交了一个new entry

c、Term3：S1进入另一个网络分区，S5当选leader，接收一个new entry；此时S5进入另一个分区，S1重新上线，S1在Term4当选leader，接收一个new entry

d、Term5：S1进入另一个分区，S5重新上线，选主，可以收到S2、3、4、5的选票，当选leader，复制entry，于是覆盖了大多数Term2的entry

所以不能数entry log中的大多数来提交entry

而是将entry提交的规则限制为，leader只提交自己本term的entry，这样可以连带着之前的旧term entry一起提交

于是，在term4时，如果有term4的entry被提交，也就是e图所示情景，那么S5将不能当选leader，因为只能赢得S4、5的选票

而如果term4没有entry commit，那么term2的entry即使复制到大多数也不会提交，那么被覆盖也就不影响正确性了

![img](https://nankai.feishu.cn/space/api/box/stream/download/asynccode/?code=ODVkNjA4NGQ3ZmIxYmJiMjYzMWM0YmZiMjYwYTViNjJfM3RsSXBXeTlSNFdEYk9UaThuUlRpNHhWNEtCc2RLRVVfVG9rZW46V2YyZGJsamtyb0ZmYk94THdldmNuMzVObk5kXzE3MzE1ODU4NzQ6MTczMTU4OTQ3NF9WNA)

4、3Property

the Log Matching Property

the Leader Completeness Property

the Log Matching Property

![img](https://nankai.feishu.cn/space/api/box/stream/download/asynccode/?code=NjlhNjU4NGUxZTNkNDYzZDYzN2JlNzI1NzQzMTYwNDZfS1h1VEpaVWl4eWcxMWlKWWJKOW16MVVtSm1JTGRib2JfVG9rZW46WEtGYWJOdjR6b21heGh4d2NoN2NLMGhmbkFkXzE3MzE1ODU4NzQ6MTczMTU4OTQ3NF9WNA)

5、Raft实现中的几个时间要求点

broadcastTime：server之间RPC包一来一回所需要的平均时间，还包括接收方的持久化时间，经典0.5-20ms

electionTimeout：选举时钟的时间，经典10ms-500ms

MTBF：单机故障的平均间隔时间

electionTimeout应该比broadcastTime大一个数量级：因为选举需要获得对方的选票，心跳包也需要进行RPC的发送

MTBF应该比electionTimeout大好几个数量级：因为系统在选举时是不可用的，希望选举时间能够占整体时间一个很小的比例

暂时无法在南开飞书文档外展示此内容

### Section7  Log compaction日志压缩

A：选用日志压缩以及快照方式来避免日志一直无限增长

Leader向follower发送InstallSnapshot RPC来实现这一点

当follower收到的快照领先于自己的日志，丢弃掉全部日志，保存快照；当自己的日志【部分】领先于快照，保留领先的部分，删除可以被快照覆盖的日志

B：快照策略的存在看起来像是违反强leader原则，因为**follower可以在leader不知情的情况下，产生快照**。

leader的存在是为了在各个实例产生冲突的时候帮助做出决策，达成一致，而快照产生的时机，不存在需要解决或者说【没有达成一致的矛盾】，因为之前leader已经在这一段日志上达成一致了。数据的流向仍然是从leader流向follower，只不过follower可以重新组织他们的数据罢了。

C：另一个可供选择的策略是：由leader来产生快照，然后通过网络发送给follower，但是这样做有两个缺陷

1、follower从自己本地生成一份快照的代价远比从网络上接收的代价低

2、这样做会加重leader的复杂性

D：有两个影响快照效率的因素

1、产生快照的频率，如果过于频繁，就会浪费磁盘带宽，因为一直在写磁盘；如果过于不频繁，就有日志增长得超过磁盘存储的风险

一个简单的策略是，日志达到规定大小时就产生快照；只要这个规定大小明显大于快照大小，那么快照的磁盘带宽开销将很小

2、产生快照是需要一定时间的，我们不希望在产生快照时影响正常的操作

采用**写时复制**技术，解决方案是使用写时复制(copy-on-write)技术，这样就可以接受新的更新，而不会影响正在写入的快照

比如我们要从log中产生快照，也有别的进程在append log entry

使用写时复制技术，可以让快照使用原始log来产生，只有在其他进程写log时才复制一份出来供产生快照的进程用；原来log的供log写入操作用，这样就不会影响正常的操作了

其实本来就是要复制一份出来提供给快照的产生的，只不过只有在写log发生时，这个复制动作才发生

### Section8 Client interaction客户端交互

使用Raft接口的理解为此处的client，也就是KV Server中的server端

A：客户端怎样连接leader

客户端刚起时，随机连接一个server；

若该server是leader则正常请求；

若该server不是leader则将告知客户端谁是leader并拒绝该请求；客户端转而连接leader

若没有leader或leader宕机则等待客户端超时，超时后进行重连

B：Raft可能会多次执行同一个客户端请求

leader执行完一个请求并提交entry后宕机，但是leader还来不及发出对应信息后宕机，客户端在不知情的情况下又向Raft提交同一个请求；则会导致raft两次提交相同的请求entry

解决方案：为每一个请求添加序列号，若同一个请求之前已经被客户端执行过，丢弃该请求

C：只读请求执行时可以不写任何信息进入raft log？但是有可能读到旧值？ 

只读请求如果接收到就立即读KV Map然后返回的话，可能会读到旧值；很早之前提交的写请求没有及时commit apply，很久之后的读请求还是没能读到该写更新【但是不影响线性一致性】

解决方案：

1、Raft leader刚当选就提交一个本term的no-op entry，让能commit的都commit了，及时apply

2、leader在回复读请求之前，需要确保自己是最新的leader；有可能新leader被选出来而当前leader不自知

raft解决这个问题是通过在应答读请求之前，raft和大多数server都交换过心跳信息了